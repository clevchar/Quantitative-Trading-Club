# -*- coding: utf-8 -*-
"""LSMC OPTIONS PRICING XPIREMENTS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OLeJ_W6KQxBMWLnNBlzyWP5KCwcdGz0b
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# ----------------- Basis (Laguerre on moneyness) -----------------
def laguerre_basis(x):
    e = np.exp(-x/2.0)
    L0 = np.ones_like(x)
    L1 = e
    L2 = e * (1.0 - x)
    L3 = e * (1.0 - 2.0*x + 0.5*x**2)
    return np.column_stack([L0, L1, L2, L3])

# ----------------- Heston + Jumps (Bates) path simulator -----------------
def simulate_bates_paths(
    S0=100.0, r=0.05, T=1.0, steps=252, paths=50_000,
    # Heston params
    v0=0.04, kappa=2.0, theta=0.04, xi=0.5, rho=-0.7,
    # Jumps (Merton): N ~ Poisson(lambdaJ*dt), log-jump ~ N(muJ, sigmaJ^2)
    lambdaJ=0.0, muJ=0.0, sigmaJ=0.0, seed=7
):
    """
    Returns S (paths x (steps+1)) simulated under risk-neutral Bates model.
    Andersen QE scheme for variance; exact Poisson for jumps each step.
    """
    if seed is not None:
        np.random.seed(seed)

    dt = T / steps
    sqrt_dt = np.sqrt(dt)

    S = np.zeros((paths, steps + 1))
    v = np.zeros((paths, steps + 1))
    S[:, 0] = S0
    v[:, 0] = max(v0, 1e-12)

    # Precompute jump drift correction so E[J] = exp(muJ + 0.5*sigmaJ^2)
    # Risk-neutral drift needs to subtract lambdaJ*(E[J]-1)
    EJm1 = np.exp(muJ + 0.5 * sigmaJ**2) - 1.0
    drift_correction = -lambdaJ * EJm1

    for t in range(steps):
        vt = v[:, t]

        # --- Variance update (QE) ---
        m = theta + (vt - theta) * np.exp(-kappa * dt)
        s2 = (vt * xi**2 * np.exp(-kappa*dt) / kappa) * (1 - np.exp(-kappa*dt)) \
             + (theta * xi**2 / (2*kappa)) * (1 - np.exp(-kappa*dt))**2
        psi = s2 / (m**2 + 1e-16)

        U = np.random.uniform(size=paths)
        v_next = np.empty_like(vt)

        # Quadratic-Exponential splitting
        psi_c = 1.5
        idx1 = psi <= psi_c
        idx2 = ~idx1

        # Case 1: noncentral chi-square approximation
        if np.any(idx1):
            b2 = 2/psi[idx1] - 1 + np.sqrt(2/psi[idx1]) * np.sqrt(2/psi[idx1] - 1)
            a  = m[idx1] / (1 + b2)
            Z  = np.random.normal(size=idx1.sum())
            v_next[idx1] = a * (Z + np.sqrt(b2))**2

        # Case 2: two-point mass approximation
        if np.any(idx2):
            p = (psi[idx2] - 1) / (psi[idx2] + 1)
            beta = (1 - p) / m[idx2]
            B = np.random.binomial(1, p, size=idx2.sum())
            v_next[idx2] = (B == 1) * 0.0 + (B == 0) * (1.0 / beta)

        v_next = np.maximum(v_next, 0.0)
        v[:, t+1] = v_next

        # --- Correlated Brownian shocks for price/variance ---
        z1 = np.random.normal(size=paths)
        z2 = np.random.normal(size=paths)
        dWv = z1 * sqrt_dt
        dWs = (rho * z1 + np.sqrt(1 - rho**2) * z2) * sqrt_dt

        # --- Jumps (Poisson with lognormal sizes) ---
        if lambdaJ > 0.0 and (muJ != 0.0 or sigmaJ != 0.0):
            N = np.random.poisson(lambdaJ * dt, size=paths)
            # If N=0 -> jump factor 1; else draw aggregate jump ~ lognormal with N*mu, N*sigma^2
            jump_factor = np.ones(paths)
            nonzero = N > 0
            if np.any(nonzero):
                Nj = N[nonzero]
                Y = np.random.normal(loc=Nj * muJ, scale=np.sqrt(Nj) * sigmaJ)
                jump_factor[nonzero] = np.exp(Y)
        else:
            N = np.zeros(paths, dtype=int)
            jump_factor = np.ones(paths)

        # --- Price update (risk-neutral drift with jump correction) ---
        S[:, t+1] = S[:, t] * np.exp(
            (r + drift_correction - 0.5 * np.maximum(vt, 0.0)) * dt
            + np.sqrt(np.maximum(vt, 0.0)) * dWs
        ) * jump_factor

    return S

# ----------------- LSMC on arbitrary paths -----------------
def lsmc_american_from_paths(S, K=100.0, r=0.05, T=1.0, option_type="put"):
    """
    Apply Longstaff–Schwartz to a pre-simulated price matrix S (paths x steps+1).
    """
    assert option_type in {"put", "call"}
    paths, cols = S.shape
    steps = cols - 1
    dt = T / steps
    disc = np.exp(-r * dt)

    if option_type == "put":
        payoff = np.maximum(K - S, 0.0)
    else:
        payoff = np.maximum(S - K, 0.0)

    cashflow = payoff[:, -1].copy()
    exercise_time = np.full(paths, steps, dtype=int)

    for t in range(steps - 1, 0, -1):
        cashflow *= disc
        itm = payoff[:, t] > 0
        if not np.any(itm):
            continue
        X = laguerre_basis(S[itm, t] / K)
        Y = cashflow[itm]
        model = LinearRegression(fit_intercept=False).fit(X, Y)
        cont = model.predict(X)

        ex = payoff[itm, t] > cont
        idx = np.where(itm)[0][ex]
        cashflow[idx] = payoff[idx, t]
        exercise_time[idx] = t

    cashflow *= disc
    price = np.mean(cashflow)
    return price, exercise_time

# ----------------- Example usage -----------------
if __name__ == "__main__":
    # Toggle models:
    # Pure GBM: set xi=0, kappa large, or just call Bates with xi=0 and lambdaJ=0
    # Heston only: lambdaJ=0
    # Merton only: xi=0, kappa large (variance ~ const theta), set lambdaJ>0

    S = simulate_bates_paths(
        S0=100, r=0.05, T=1.0, steps=100, paths=50_000,
        v0=0.04, kappa=2.0, theta=0.04, xi=0.5, rho=-0.7,
        lambdaJ=0.5, muJ=-0.10, sigmaJ=0.25,  # average downward jump with dispersion
        seed=1
    )
    price, ex_times = lsmc_american_from_paths(S, K=100, r=0.05, T=1.0, option_type="put")
    print(f"Bates (Heston + Jumps) American put price: {price:.4f}")

    # Quick visualization of a few paths and exercise marks
    show = 25
    t = np.arange(S.shape[1])
    plt.figure(figsize=(11,7))
    for i in range(show):
        plt.plot(t, S[i], lw=1)
        tt = ex_times[i]
        if tt < S.shape[1]:
            plt.scatter(tt, S[i, tt], s=60, marker="X")
    plt.title("Sample Bates Paths with Early Exercise Marks (American Put)")
    plt.xlabel("Step"); plt.ylabel("Price"); plt.grid(True); plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# ====== From our Bates LSMC code ======
def laguerre_basis(x):
    e = np.exp(-x/2.0)
    return np.column_stack([np.ones_like(x), e, e*(1-x), e*(1-2*x+0.5*x**2)])

def simulate_bates_paths(
    S0=100.0, r=0.05, T=1.0, steps=100, paths=20000,
    v0=0.04, kappa=2.0, theta=0.04, xi=0.5, rho=-0.7,
    lambdaJ=0.5, muJ=-0.10, sigmaJ=0.25, seed=42
):
    np.random.seed(seed)
    dt = T / steps
    sqrt_dt = np.sqrt(dt)
    S = np.zeros((paths, steps + 1))
    v = np.zeros((paths, steps + 1))
    S[:, 0], v[:, 0] = S0, v0
    EJm1 = np.exp(muJ + 0.5 * sigmaJ**2) - 1.0
    drift_corr = -lambdaJ * EJm1

    for t in range(steps):
        vt = v[:, t]
        m = theta + (vt - theta) * np.exp(-kappa*dt)
        s2 = (vt * xi**2 * np.exp(-kappa*dt) / kappa) * (1 - np.exp(-kappa*dt)) \
             + (theta * xi**2 / (2*kappa)) * (1 - np.exp(-kappa*dt))**2
        psi = s2 / (m**2 + 1e-16)
        v_next = np.empty_like(vt)
        psi_c = 1.5
        idx1 = psi <= psi_c
        idx2 = ~idx1
        if np.any(idx1):
            b2 = 2/psi[idx1] - 1 + np.sqrt(2/psi[idx1]) * np.sqrt(2/psi[idx1] - 1)
            a = m[idx1] / (1 + b2)
            Z = np.random.normal(size=idx1.sum())
            v_next[idx1] = a * (Z + np.sqrt(b2))**2
        if np.any(idx2):
            p = (psi[idx2] - 1) / (psi[idx2] + 1)
            beta = (1 - p) / m[idx2]
            B = np.random.binomial(1, p, size=idx2.sum())
            v_next[idx2] = (B == 1)*0 + (B == 0)*(1/beta)
        v[:, t+1] = np.maximum(v_next, 0.0)

        z1, z2 = np.random.normal(size=paths), np.random.normal(size=paths)
        dWv = z1 * sqrt_dt
        dWs = (rho*z1 + np.sqrt(1 - rho**2)*z2) * sqrt_dt

        if lambdaJ > 0:
            N = np.random.poisson(lambdaJ*dt, size=paths)
            jump_factor = np.ones(paths)
            nz = N > 0
            if np.any(nz):
                Nj = N[nz]
                Y = np.random.normal(Nj*muJ, np.sqrt(Nj)*sigmaJ)
                jump_factor[nz] = np.exp(Y)
        else:
            jump_factor = np.ones(paths)

        S[:, t+1] = S[:, t] * np.exp(
            (r + drift_corr - 0.5*np.maximum(vt, 0))*dt
            + np.sqrt(np.maximum(vt, 0))*dWs
        ) * jump_factor
    return S

def lsmc_american_put_from_paths(S, K=100.0, r=0.05, T=1.0):
    steps = S.shape[1] - 1
    dt = T / steps
    disc = np.exp(-r * dt)
    payoff = np.maximum(K - S, 0.0)
    cashflow = payoff[:, -1].copy()
    exercise_flags = np.zeros_like(S)

    for t in range(steps - 1, 0, -1):
        cashflow *= disc
        itm = payoff[:, t] > 0
        if not np.any(itm): continue
        X, Y = S[itm, t]/K, cashflow[itm]
        A = laguerre_basis(X)
        model = LinearRegression(fit_intercept=False).fit(A, Y)
        cont = model.predict(A)
        immediate = payoff[itm, t]
        ex = immediate > cont
        idx = np.where(itm)[0][ex]
        cashflow[idx] = immediate[ex]
        exercise_flags[idx, t] = 1
    return exercise_flags

# ====== Heatmap of log-moneyness vs time ======
def plot_log_moneyness_heatmap(S, flags, K, T):
    steps = S.shape[1] - 1
    times = np.linspace(0, T, steps + 1)
    log_m = np.log(S / K)
    lm_flat, t_flat = log_m.flatten(), np.tile(times, S.shape[0])
    ex_flat = flags.flatten()
    lm_ex, t_ex = lm_flat[ex_flat == 1], t_flat[ex_flat == 1]
    heatmap, xedges, yedges = np.histogram2d(
        lm_ex, t_ex, bins=[100, 100], range=[[-1, 1], [0, T]]
    )
    plt.figure(figsize=(10, 6))
    plt.title(f"Optimal Stopping Heatmap (log(S/K)) — Bates Model")
    plt.xlabel("log(S / K)")
    plt.ylabel("Time to Maturity")
    plt.imshow(
        heatmap.T, origin="lower", aspect="auto",
        extent=[-1, 1, 0, T], cmap="hot"
    )
    plt.colorbar(label="Exercise Frequency")
    plt.grid(False)
    plt.show()

# ====== Run simulation and plot ======
S = simulate_bates_paths(
    S0=100, r=0.05, T=1.0, steps=100, paths=20000,
    v0=0.04, kappa=2.0, theta=0.04, xi=0.5, rho=-0.7,
    lambdaJ=0.5, muJ=-0.10, sigmaJ=0.25
)
flags = lsmc_american_put_from_paths(S, K=100, r=0.05, T=1.0)
plot_log_moneyness_heatmap(S, flags, K=100, T=1.0)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.kernel_ridge import KernelRidge
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401
import math

# =============================
# Basis (Laguerre on moneyness)
# =============================
def laguerre_basis(x):
    e = np.exp(-x/2.0)
    return np.column_stack([
        np.ones_like(x),
        e,
        e*(1.0 - x),
        e*(1.0 - 2.0*x + 0.5*x**2)
    ])

# =========================================
# Bates (Heston + Merton jumps) path engine
# =========================================
def simulate_bates_paths(
    S0=100.0, r=0.05, T=2.0, steps=200, paths=50_000,
    v0=0.04, kappa=2.0, theta=0.04, xi=0.5, rho=-0.50,
    lambdaJ=0.5, muJ=-0.10, sigmaJ=0.25, seed=1
):
    if seed is not None:
        np.random.seed(seed)
    dt = T/steps
    sqrt_dt = math.sqrt(dt)

    S = np.zeros((paths, steps+1))
    v = np.zeros((paths, steps+1))
    S[:,0], v[:,0] = S0, max(v0, 1e-12)

    EJm1 = math.exp(muJ + 0.5*sigmaJ**2) - 1.0
    drift_corr = -lambdaJ * EJm1

    for t in range(steps):
        vt = v[:, t]
        # Andersen QE for variance
        m  = theta + (vt - theta) * math.exp(-kappa*dt)
        s2 = (vt*xi**2*math.exp(-kappa*dt)/kappa)*(1 - math.exp(-kappa*dt)) \
             + (theta*xi**2/(2*kappa))*(1 - math.exp(-kappa*dt))**2
        psi = s2 / (m**2 + 1e-16)

        v_next = np.empty_like(vt)
        psi_c = 1.5
        idx1 = psi <= psi_c
        idx2 = ~idx1
        if np.any(idx1):
            b2 = 2/psi[idx1] - 1 + np.sqrt(2/psi[idx1]) * np.sqrt(2/psi[idx1]-1)
            a  = m[idx1] / (1 + b2)
            Z  = np.random.normal(size=idx1.sum())
            v_next[idx1] = a * (Z + np.sqrt(b2))**2
        if np.any(idx2):
            p = (psi[idx2]-1)/(psi[idx2]+1)
            beta = (1-p)/m[idx2]
            B = np.random.binomial(1, p, size=idx2.sum())
            v_next[idx2] = (B==1)*0.0 + (B==0)*(1.0/beta)
        v[:, t+1] = np.maximum(v_next, 0.0)

        # correlated Brownian shocks
        z1 = np.random.normal(size=paths)
        z2 = np.random.normal(size=paths)
        dWs = (rho*z1 + np.sqrt(1-rho**2)*z2) * sqrt_dt

        # jumps
        if lambdaJ > 0:
            N = np.random.poisson(lambdaJ*dt, size=paths)
            jump_factor = np.ones(paths)
            nz = N > 0
            if np.any(nz):
                Nj = N[nz]
                Y = np.random.normal(Nj*muJ, np.sqrt(Nj)*sigmaJ)
                jump_factor[nz] = np.exp(Y)
        else:
            jump_factor = np.ones(paths)

        S[:, t+1] = S[:, t] * np.exp(
            (r + drift_corr - 0.5*np.maximum(vt,0))*dt
            + np.sqrt(np.maximum(vt,0))*dWs
        ) * jump_factor
    return S

# =========================================================
# LSMC (American put) that stores a model per time slice
# =========================================================
def lsmc_put_with_models(S, K=100.0, r=0.05, T=2.0, max_itm_sample=None):
    """
    Fit continuation on ITM (stability), decide exercise, store a ridge model per t.
    Returns: price, exercise_time, models (list of dicts or None per t).
    """
    n, cols = S.shape
    steps = cols - 1
    dt = T/steps
    disc = math.exp(-r*dt)

    payoff = np.maximum(K - S, 0.0)
    cashflow = payoff[:, -1].copy()
    exercise_time = np.full(n, steps, dtype=int)
    models = [None]*(steps+1)

    for t in range(steps-1, 0, -1):
        cashflow *= disc
        itm = payoff[:, t] > 0
        if not np.any(itm):
            models[t] = None
            continue

        # optional cap on ITM sample for the regression
        if max_itm_sample is not None and itm.sum() > max_itm_sample:
            idx = np.where(itm)[0]
            keep = np.random.choice(idx, size=max_itm_sample, replace=False)
            fit_mask = np.zeros_like(itm, dtype=bool); fit_mask[keep] = True
        else:
            fit_mask = itm

        Xm = (S[fit_mask, t] / K)
        Ym = cashflow[fit_mask]
        A = laguerre_basis(Xm)
        reg = Ridge(alpha=1e-6, fit_intercept=False).fit(A, Ym)

        # decide exercise on ITM using the fitted model
        cont_itm = reg.predict(laguerre_basis(S[itm, t]/K))
        imm_itm  = payoff[itm, t]
        exercise = imm_itm > cont_itm
        if np.any(exercise):
            ex_idx = np.where(itm)[0][exercise]
            cashflow[ex_idx] = imm_itm[exercise]
            exercise_time[ex_idx] = t

        models[t] = {"coef": reg.coef_.copy()}
    cashflow *= disc
    price = float(np.mean(cashflow))
    return price, exercise_time, models

# ========================================================
# Sampled pre-execution cloud (temporal + spatial thinning)
# ========================================================
def build_sampled_cloud(
    S, K, T, exercise_time, models,
    sample_every=5,                 # take every k-th time slice
    per_time_cap=5000,              # max points per sampled slice
    stratify_bins=(-0.9,-0.6,-0.3,-0.1,0.1,0.3,0.6,0.9),  # log(S/K) bins
    seed=123
):
    rng = np.random.default_rng(seed)
    n, cols = S.shape
    steps = cols - 1
    dt = T/steps

    S_cloud, tau_cloud, V_cloud = [], [], []

    for t in range(1, steps, sample_every):
        model = models[t]
        if model is None:
            continue
        alive = t < exercise_time
        if not np.any(alive):
            continue

        S_t = S[alive, t]
        A   = laguerre_basis(S_t / K)
        cont = A @ model["coef"]            # continuation at time t
        m_t = np.log(S_t / K)

        # stratified sampling across log-moneyness
        idx_alive = np.where(alive)[0]
        pick_idx = np.arange(idx_alive.size)
        if stratify_bins is not None:
            bins = np.array(stratify_bins)
            bucket = np.digitize(m_t, bins)  # 0..len(bins)
            picks = []
            uniq = np.unique(bucket)
            per_bin = max(1, per_time_cap // max(1, uniq.size))
            for b in uniq:
                in_bin = np.where(bucket == b)[0]
                if in_bin.size <= per_bin:
                    picks.append(in_bin)
                else:
                    picks.append(rng.choice(in_bin, size=per_bin, replace=False))
            pick_idx = np.concatenate(picks)
        else:
            if pick_idx.size > per_time_cap:
                pick_idx = rng.choice(pick_idx, size=per_time_cap, replace=False)

        S_cloud.append(S_t[pick_idx])
        tau_cloud.append(np.full(pick_idx.size, T - t*dt))
        V_cloud.append(cont[pick_idx])

    if len(S_cloud)==0:
        return np.array([]), np.array([]), np.array([])
    return np.concatenate(S_cloud), np.concatenate(tau_cloud), np.concatenate(V_cloud)

# ==============================================
# Black–Scholes put, Vega, robust IV inversion
# ==============================================
def bs_put_price(S, K, r, tau, sigma):
    if tau <= 0: return max(K - S, 0.0)
    if sigma <= 0: return max(K*math.exp(-r*tau) - S, 0.0)
    d1 = (math.log(S/K) + (r + 0.5*sigma*sigma)*tau) / (sigma*math.sqrt(tau))
    d2 = d1 - sigma*math.sqrt(tau)
    N = lambda x: 0.5*(1.0 + math.erf(x/math.sqrt(2.0)))
    return K*math.exp(-r*tau)*N(-d2) - S*N(-d1)

def bs_put_vega(S, K, r, tau, sigma):
    if tau <= 0 or sigma <= 0: return 0.0
    d1 = (math.log(S/K) + (r + 0.5*sigma*sigma)*tau) / (sigma*math.sqrt(tau))
    n = math.exp(-0.5*d1*d1)/math.sqrt(2*math.pi)
    return S*math.sqrt(tau)*n

def implied_vol_put_bisect_safe(S, K, r, tau, target, tol=1e-6, iters=80):
    p_min = max(K*math.exp(-r*tau) - S, 0.0)
    if tau <= 0 or target <= p_min + 1e-12:
        return float("nan")
    lo, hi = 1e-8, 5.0
    for _ in range(iters):
        mid = 0.5*(lo+hi)
        price = bs_put_price(S, K, r, tau, mid)
        if abs(price - target) < tol:
            if bs_put_vega(S, K, r, tau, max(mid,1e-8)) < 1e-5:
                return float("nan")
            return mid
        if price > target: hi = mid
        else: lo = mid
    return float("nan")

def build_iv_cloud(S_cloud, tau_cloud, V_cloud, K=100.0, r=0.05):
    iv, m, t = [], [], []
    for S, tau, cont in zip(S_cloud, tau_cloud, V_cloud):
        if tau < 0.03:        # near-expiry is numerically unstable
            continue
        intrinsic = max(K - S, 0.0)
        cushion = max(1e-3, 0.005*K*math.sqrt(tau))  # require distance from intrinsic
        if cont - intrinsic < cushion:
            continue
        iv_val = implied_vol_put_bisect_safe(S, K, r, tau, cont)
        if not (iv_val > 0 and iv_val < 2.0):
            continue
        iv.append(iv_val); m.append(math.log(S/K)); t.append(tau)
    return np.array(m), np.array(t), np.array(iv)

# =========================================
# Surface fitters over the IV cloud (m,T)
# =========================================
def fit_iv_surface(m, T, iv,
                   method="poly",          # "poly" or "rbf"
                   poly_degree=2,
                   rbf_gamma=6.0,          # ~1/(lengthscale^2)
                   alpha=1e-3,             # ridge strength (for rbf)
                   grid_m_range=(-0.9, 0.9),
                   grid_t_range=(0.03, 2.0),
                   grid_m_bins=120,
                   grid_t_bins=140):
    m_grid = np.linspace(*grid_m_range, grid_m_bins)
    t_grid = np.linspace(*grid_t_range, grid_t_bins)
    M, TT = np.meshgrid(m_grid, t_grid, indexing="xy")

    X = np.column_stack([m, T])
    Xg = np.column_stack([M.ravel(), TT.ravel()])

    if method == "poly":
        Phi = PolynomialFeatures(degree=poly_degree, include_bias=True)
        Xp  = Phi.fit_transform(X)
        Xgp = Phi.transform(Xg)
        reg = LinearRegression().fit(Xp, iv)
        iv_fit = reg.predict(Xgp).reshape(M.shape)

    elif method == "rbf":
        scaler = StandardScaler().fit(X)
        Xs  = scaler.transform(X)
        Xgs = scaler.transform(Xg)
        kr = KernelRidge(kernel="rbf", gamma=rbf_gamma, alpha=alpha)
        kr.fit(Xs, iv)
        iv_fit = kr.predict(Xgs).reshape(M.shape)
    else:
        raise ValueError("method must be 'poly' or 'rbf'")

    return M, TT, iv_fit

def plot_surface_from_cloud(m, T, iv, M, TT, IV_fit, title):
    fig = plt.figure(figsize=(12,7))
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(T, m, iv, s=4, alpha=0.12, label="IV cloud")
    ax.plot_surface(TT, M, IV_fit, linewidth=0, antialiased=True, alpha=0.92, cmap="viridis")
    ax.set_xlabel("Time to Expiry (years)")
    ax.set_ylabel("log(S/K)")
    ax.set_zlabel("Implied Volatility")
    ax.set_title(title)
    plt.tight_layout()
    plt.show()

# ==================
# Main pipeline
# ==================
if __name__ == "__main__":
    # --- Simulate longer maturities + milder leverage
    K=100; r=0.05; T=2.0
    S = simulate_bates_paths(
        S0=100, r=r, T=T, steps=200, paths=50_000,
        v0=0.04, kappa=2.0, theta=0.04, xi=0.5, rho=-0.50,
        lambdaJ=0.5, muJ=-0.10, sigmaJ=0.25, seed=1
    )

    # --- LSMC with per-time models
    price, ex_times, models = lsmc_put_with_models(S, K=K, r=r, T=T, max_itm_sample=20000)
    print(f"American put price (Bates LSMC): {price:.4f}")

    # --- Sampled pre-execution cloud (every 5 steps, stratified by moneyness)
    S_c, tau_c, V_c = build_sampled_cloud(
        S, K, T, ex_times, models,
        sample_every=5, per_time_cap=5000,
        stratify_bins=(-0.9,-0.6,-0.3,-0.1,0.1,0.3,0.6,0.9), seed=123
    )
    print("Sampled continuation points:", len(S_c))

    # --- Invert to IV (robust filters inside)
    m, t, iv = build_iv_cloud(S_c, tau_c, V_c, K=K, r=r)

    # --- Fit & plot: Polynomial (toy-like, stable)
    M, TT, IV_poly = fit_iv_surface(
        m, t, iv,
        method="poly", poly_degree=2,
        grid_m_range=(-0.9, 0.9), grid_t_range=(0.05, 2.0)
    )
    plot_surface_from_cloud(m, t, iv, M, TT, IV_poly,
                            title="IV surface fit: Polynomial degree 2")

    # --- Fit & plot: RBF kernel ridge (smoother, more flexible)
    M, TT, IV_rbf = fit_iv_surface(
        m, t, iv,
        method="rbf", rbf_gamma=6.0, alpha=1e-3,
        grid_m_range=(-0.9, 0.9), grid_t_range=(0.05, 2.0)
    )
    plot_surface_from_cloud(m, t, iv, M, TT, IV_rbf,
                            title="IV surface fit: RBF kernel ridge")